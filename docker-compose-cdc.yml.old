version: "3.8"

services:
  # ================= Kafka + Debezium + Schema Registry =================
  zookeeper:
    image: confluentinc/cp-zookeeper:5.5.3
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-enterprise-kafka:5.5.3
    links:
      - zookeeper
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9991
    ports:
      - 9092:9092

  mysql:
    image: quay.io/debezium/example-mysql:1.9
    ports:
      - 3306:3306
    environment:
      MYSQL_ROOT_PASSWORD: debezium
      MYSQL_USER: mysqluser
      MYSQL_PASSWORD: mysqlpw
      MYSQL_DATABASE: stroke_predictions

  connect:
    image: quay.io/debezium/connect:1.9
    ports:
      - 8083:8083
    links:
      - kafka
      - mysql
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: my_connect_configs
      OFFSET_STORAGE_TOPIC: my_connect_offsets
      STATUS_STORAGE_TOPIC: my_connect_statuses
    volumes:
      - ./kconnect-jdbc-sink-jars:/kafka/connect/kconnect-jdbc-sink-jars
      - ./kconnect-s3-sink-jars:/kafka/connect/kconnect-s3-sink-jars
      - ./confluentic-connect-transforms:/kafka/connect/confluentic-connect-transforms

  schema-registry:
    image: confluentinc/cp-schema-registry:5.5.3
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8081,http://localhost:8081
    ports:
      - 8081:8081
    depends_on:
      - zookeeper
      - kafka

  # ================= Stroke Prediction API =================
  stroke-prediction-api:
    build: .
    container_name: stroke-prediction-api
    ports:
      - "8000:8000"
    volumes:
      - ./fast_api:/app/fast_api
      - ./models:/app/models
      - ./.env:/app/fast_api/.env
    working_dir: /app/fast_api
    depends_on:
      - kafka
      - mysql

  # ================= Spark Cluster + PySpark Notebook =================
  spark:
    image: bitnami/spark:3.5.1
    hostname: spark
    container_name: spark-master
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
    ports:
      - "7077:7077" # Spark master
      - "8080:8080" # Spark UI
    networks:
      - spark

  spark-worker-1:
    image: bitnami/spark:3.5.1
    container_name: spark-worker-1
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark:7077
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
    depends_on:
      - spark
    networks:
      - spark

  pyspark:
    image: jupyter/pyspark-notebook:latest
    container_name: pyspark
    hostname: pyspark
    ports:
      - "9999:8888" # Jupyter Notebook UI
    volumes:
      - ./notebooks:/home/jovyan/work
    environment:
      JUPYTER_ENABLE_LAB: "no"
      PYSPARK_SUBMIT_ARGS: --packages org.apache.spark:spark-avro_2.12:3.5.1,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 pyspark-shell
      AWS_ACCESS_KEY_ID: "xx"
      AWS_SECRET_ACCESS_KEY: "xx"
      AWS_DEFAULT_REGION: us-east-1
    networks:
      - spark
    depends_on:
      - spark

# ================= Networks =================
networks:
  spark:
    driver: bridge
